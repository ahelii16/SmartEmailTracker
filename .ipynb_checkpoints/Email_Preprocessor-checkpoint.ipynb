{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1. How to work with emoji package\n",
    "#2. How to process csv files or custom datasets(not keras dataset)\n",
    "#3. How to use transfer learning\n",
    "#4. Build an LSTM model\n",
    "#5. Stacked LSTM\n",
    "#6. Predictions (i/p: sentence  o/p: most suitable emoji)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, email,re\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns; sns.set_style('whitegrid')\n",
    "\n",
    "# NLP\n",
    "from nltk.tokenize.regexp import RegexpTokenizer\n",
    "from subprocess import check_output\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer,CountVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "import gensim\n",
    "from gensim import corpora\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "import string\n",
    "from nltk.stem.porter import PorterStemmer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(300, 6)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>From</th>\n",
       "      <th>To</th>\n",
       "      <th>Subject</th>\n",
       "      <th>Body</th>\n",
       "      <th>Class(output)</th>\n",
       "      <th>Payment Date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Adam@HSBC.com</td>\n",
       "      <td>Dipesh@CitiBankLondon.com</td>\n",
       "      <td>Urgent : need transaction details for 845967</td>\n",
       "      <td>We have received your request and will be send...</td>\n",
       "      <td>D</td>\n",
       "      <td>2020-04-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Phillip@CitiBankLondon.com</td>\n",
       "      <td>Dipesh@CitiBankHingKong.com</td>\n",
       "      <td>Payment Failed for 572172</td>\n",
       "      <td>Payment of USD 188399 has been cancelled as pe...</td>\n",
       "      <td>A</td>\n",
       "      <td>2019-05-10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Mike@DEUTSCHEBANK.com</td>\n",
       "      <td>Anthony@CitiBankLondon.com</td>\n",
       "      <td>Update on payment request for 296938</td>\n",
       "      <td>Kindly provide the details for Acc no. 296938 ...</td>\n",
       "      <td>D</td>\n",
       "      <td>2019-04-29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Morgan@DEUTSCHEBANK.com</td>\n",
       "      <td>Dipesh@CitiBankPune.com</td>\n",
       "      <td>Update on payment request for 942151</td>\n",
       "      <td>Please find the details of your account 942151...</td>\n",
       "      <td>D</td>\n",
       "      <td>2019-09-16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Spike@CitiBankNewYork.com</td>\n",
       "      <td>Shai@CitiBankNewYork.com</td>\n",
       "      <td>Transaction Failed for 537968</td>\n",
       "      <td>Due to lack of funds, cheque no. 2755223926952...</td>\n",
       "      <td>A</td>\n",
       "      <td>2019-03-06</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         From                           To  \\\n",
       "0               Adam@HSBC.com    Dipesh@CitiBankLondon.com   \n",
       "1  Phillip@CitiBankLondon.com  Dipesh@CitiBankHingKong.com   \n",
       "2       Mike@DEUTSCHEBANK.com   Anthony@CitiBankLondon.com   \n",
       "3     Morgan@DEUTSCHEBANK.com      Dipesh@CitiBankPune.com   \n",
       "4   Spike@CitiBankNewYork.com     Shai@CitiBankNewYork.com   \n",
       "\n",
       "                                        Subject  \\\n",
       "0  Urgent : need transaction details for 845967   \n",
       "1                     Payment Failed for 572172   \n",
       "2          Update on payment request for 296938   \n",
       "3          Update on payment request for 942151   \n",
       "4                 Transaction Failed for 537968   \n",
       "\n",
       "                                                Body Class(output)  \\\n",
       "0  We have received your request and will be send...             D   \n",
       "1  Payment of USD 188399 has been cancelled as pe...             A   \n",
       "2  Kindly provide the details for Acc no. 296938 ...             D   \n",
       "3  Please find the details of your account 942151...             D   \n",
       "4  Due to lack of funds, cheque no. 2755223926952...             A   \n",
       "\n",
       "  Payment Date  \n",
       "0   2020-04-04  \n",
       "1   2019-05-10  \n",
       "2   2019-04-29  \n",
       "3   2019-09-16  \n",
       "4   2019-03-06  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# emails_df = pd.read_csv('./emails.csv', nrows=20000)\n",
    "df = pd.read_csv('./emaildataset.csv')\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Class'] = df['Class(output)']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The transaction 122775 is taking too long to be completed\n",
      "B\n"
     ]
    }
   ],
   "source": [
    "print(df.loc[15][\"Body\"])\n",
    "print(df.loc[15][\"Class\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def clean(text):\n",
    "#     stop = set(stopwords.words('english'))\n",
    "#     stop.update((\"to\",\"cc\",\"subject\",\"http\",\"from\",\"sent\",\"thanks\", \"regards\", \"hi\", \"hello\", \"thank you\"))\n",
    "#     exclude = set(string.punctuation) \n",
    "#     lemma = WordNetLemmatizer()\n",
    "#     porter= PorterStemmer()\n",
    "    \n",
    "#     text=text.rstrip()\n",
    "#     text = re.sub(r'[^a-zA-Z]', ' ', text)\n",
    "#     amount = \" \".join([i for i in text.lower().split() if i.isdigit()])\n",
    "#     stop_free = \" \".join([i for i in text.lower().split() if((i not in stop) and (not i.isdigit()))])\n",
    "#     punc_free = ''.join(ch for ch in stop_free if ch not in exclude)\n",
    "#     normalized = \" \".join(lemma.lemmatize(word) for word in punc_free.split())\n",
    "#     stem = \" \".join(porter.stem(token) for token in normalized.split())\n",
    "    \n",
    "#     return normalized, amount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## sub_df[\"content\"]=sub_df[\"content\"].map(clean)\n",
    "# text_clean=[]\n",
    "\n",
    "# for text in df['Body']:\n",
    "#     text_clean.append(clean(text)[0].split())\n",
    "#     amount = clean(text)[1]\n",
    "\n",
    "# amount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = {      \"1\": \"Transaction complete\",\n",
    "                    \"2\": \"Payment is in progress\",\n",
    "                    \"3\": \"Payment Failed\",\n",
    "                    \"4\": \"Status check\",\n",
    "                   }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we are going to use these 5 classes for sentiment analysis o/p (5 o/p classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Processing a custom dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print the sentences with emojis\n",
    "data = df.values\n",
    "for i in range(df.shape[0]):\n",
    "    # merge subject and body strings\n",
    "    df['text_data'] = df['Subject'] + \" \" + df['Body']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert sentences to embeddings and classes to one-hot vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "400000\n"
     ]
    }
   ],
   "source": [
    "embeddings = {}\n",
    "with open('../data_science_CB/Machine-learning-online-2018-master/17. Word2Vec/glove.6B.50d.txt',encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coeffs = np.asarray(values[1:],dtype='float32')\n",
    "        \n",
    "#         print(word)\n",
    "#         print(coeffs)\n",
    "        embeddings[word] = coeffs\n",
    "    f.close()\n",
    "print(len(embeddings))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4 - Converting sentences to vectors (creating the o/p of embedding layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getOutputEmbeddings(X):\n",
    "    \n",
    "    #let 10 is max len of sentence, 50 batch size (no. of e.g.)\n",
    "    embedding_matrix_output = np.zeros((X.shape[0],100,50))\n",
    "    \n",
    "    for ix in range(X.shape[0]):\n",
    "        X[ix] = X[ix].split()\n",
    "        for jx in range(len(X[ix])):\n",
    "            #go to every word in current(ix) sentence\n",
    "            embedding_matrix_output[ix][jx] = embeddings[X[ix][jx].lower() and not(X[ix][jx].isDigit())]\n",
    "            \n",
    "    return embedding_matrix_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Urgent : need transaction details for 845967 We have received your request and will be sending you the details of acc 845967 soon.\n",
      "(300,)\n"
     ]
    }
   ],
   "source": [
    "XT = df['text_data']\n",
    "\n",
    "print(XT[0])\n",
    "print(XT.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "YT = to_categorical(df['Class'], num_classes=4)\n",
    "# Yt = to_categorical(test[1], num_classes=5)\n",
    "\n",
    "#print(Xt.shape)\n",
    "# print(YT.shape)\n",
    "# print(Yt.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'split'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-5a3ce4b70a1d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0membed_matrix_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetOutputEmbeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;31m#embed_matrix_test = getOutputEmbeddings(Xt)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXT\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-20-b23def529d16>\u001b[0m in \u001b[0;36mgetOutputEmbeddings\u001b[0;34m(X)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mix\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m         \u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mix\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mix\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mjx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mix\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m             \u001b[0;31m#go to every word in current(ix) sentence\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'split'"
     ]
    }
   ],
   "source": [
    "embed_matrix_train = getOutputEmbeddings(XT)\n",
    "#embed_matrix_test = getOutputEmbeddings(Xt)\n",
    "\n",
    "print(XT[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(132, 10, 50)\n",
      "(56, 10, 50)\n"
     ]
    }
   ],
   "source": [
    "print(embed_matrix_train.shape)\n",
    "print(embed_matrix_test.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Define RNN/LSTM model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import *\n",
    "from keras.models import Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_5 (LSTM)                (None, 10, 64)            29440     \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 10, 64)            0         \n",
      "_________________________________________________________________\n",
      "lstm_6 (LSTM)                (None, 64)                33024     \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 5)                 325       \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 5)                 0         \n",
      "=================================================================\n",
      "Total params: 62,789\n",
      "Trainable params: 62,789\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(64,input_shape=(10,50),return_sequences=True)) #hidden state: 64 dim\n",
    "model.add(Dropout(0.4))\n",
    "model.add(LSTM(64,input_shape=(10,50), return_sequences=False))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(5))\n",
    "model.add(Activation('softmax'))\n",
    "model.compile(optimizer='adam',loss='categorical_crossentropy',metrics=['acc'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAIN MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 105 samples, validate on 27 samples\n",
      "Epoch 1/40\n",
      "105/105 [==============================] - 1s 9ms/step - loss: 1.5876 - acc: 0.2571 - val_loss: 1.6177 - val_acc: 0.1852\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.61768, saving model to best_model.h5\n",
      "Epoch 2/40\n",
      "105/105 [==============================] - 0s 529us/step - loss: 1.5314 - acc: 0.3238 - val_loss: 1.6585 - val_acc: 0.2963\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 1.61768\n",
      "Epoch 3/40\n",
      "105/105 [==============================] - 0s 672us/step - loss: 1.4945 - acc: 0.3429 - val_loss: 1.6792 - val_acc: 0.2222\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1.61768\n",
      "Epoch 4/40\n",
      "105/105 [==============================] - 0s 551us/step - loss: 1.4661 - acc: 0.3714 - val_loss: 1.6625 - val_acc: 0.2222\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1.61768\n",
      "Epoch 5/40\n",
      "105/105 [==============================] - 0s 956us/step - loss: 1.4012 - acc: 0.4190 - val_loss: 1.6090 - val_acc: 0.1852\n",
      "\n",
      "Epoch 00005: val_loss improved from 1.61768 to 1.60899, saving model to best_model.h5\n",
      "Epoch 6/40\n",
      "105/105 [==============================] - 0s 574us/step - loss: 1.3755 - acc: 0.4000 - val_loss: 1.5418 - val_acc: 0.2222\n",
      "\n",
      "Epoch 00006: val_loss improved from 1.60899 to 1.54177, saving model to best_model.h5\n",
      "Epoch 7/40\n",
      "105/105 [==============================] - 0s 617us/step - loss: 1.3142 - acc: 0.4857 - val_loss: 1.4655 - val_acc: 0.2963\n",
      "\n",
      "Epoch 00007: val_loss improved from 1.54177 to 1.46553, saving model to best_model.h5\n",
      "Epoch 8/40\n",
      "105/105 [==============================] - 0s 770us/step - loss: 1.2251 - acc: 0.5524 - val_loss: 1.3675 - val_acc: 0.4074\n",
      "\n",
      "Epoch 00008: val_loss improved from 1.46553 to 1.36747, saving model to best_model.h5\n",
      "Epoch 9/40\n",
      "105/105 [==============================] - 0s 590us/step - loss: 1.1518 - acc: 0.5810 - val_loss: 1.3275 - val_acc: 0.3704\n",
      "\n",
      "Epoch 00009: val_loss improved from 1.36747 to 1.32752, saving model to best_model.h5\n",
      "Epoch 10/40\n",
      "105/105 [==============================] - 0s 584us/step - loss: 1.0480 - acc: 0.6000 - val_loss: 1.3779 - val_acc: 0.3704\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 1.32752\n",
      "Epoch 11/40\n",
      "105/105 [==============================] - 0s 689us/step - loss: 0.9343 - acc: 0.6476 - val_loss: 1.3410 - val_acc: 0.4444\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 1.32752\n",
      "Epoch 12/40\n",
      "105/105 [==============================] - 0s 580us/step - loss: 0.8433 - acc: 0.7238 - val_loss: 1.2717 - val_acc: 0.5185\n",
      "\n",
      "Epoch 00012: val_loss improved from 1.32752 to 1.27169, saving model to best_model.h5\n",
      "Epoch 13/40\n",
      "105/105 [==============================] - 0s 518us/step - loss: 0.8054 - acc: 0.7143 - val_loss: 1.2474 - val_acc: 0.5556\n",
      "\n",
      "Epoch 00013: val_loss improved from 1.27169 to 1.24738, saving model to best_model.h5\n",
      "Epoch 14/40\n",
      "105/105 [==============================] - 0s 713us/step - loss: 0.7208 - acc: 0.7333 - val_loss: 1.3253 - val_acc: 0.4815\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 1.24738\n",
      "Epoch 15/40\n",
      "105/105 [==============================] - 0s 696us/step - loss: 0.6575 - acc: 0.7333 - val_loss: 1.3249 - val_acc: 0.5185\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 1.24738\n",
      "Epoch 16/40\n",
      "105/105 [==============================] - 0s 964us/step - loss: 0.5818 - acc: 0.7905 - val_loss: 1.1242 - val_acc: 0.5926\n",
      "\n",
      "Epoch 00016: val_loss improved from 1.24738 to 1.12419, saving model to best_model.h5\n",
      "Epoch 17/40\n",
      "105/105 [==============================] - 0s 600us/step - loss: 0.6440 - acc: 0.7714 - val_loss: 0.9099 - val_acc: 0.7037\n",
      "\n",
      "Epoch 00017: val_loss improved from 1.12419 to 0.90987, saving model to best_model.h5\n",
      "Epoch 18/40\n",
      "105/105 [==============================] - 0s 555us/step - loss: 0.5117 - acc: 0.8381 - val_loss: 0.9997 - val_acc: 0.5926\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.90987\n",
      "Epoch 19/40\n",
      "105/105 [==============================] - 0s 584us/step - loss: 0.4319 - acc: 0.8476 - val_loss: 1.1607 - val_acc: 0.6296\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.90987\n",
      "Epoch 20/40\n",
      "105/105 [==============================] - 0s 695us/step - loss: 0.3854 - acc: 0.8381 - val_loss: 1.0874 - val_acc: 0.7037\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.90987\n",
      "Epoch 21/40\n",
      "105/105 [==============================] - 0s 697us/step - loss: 0.3552 - acc: 0.8571 - val_loss: 1.0247 - val_acc: 0.6667\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.90987\n",
      "Epoch 22/40\n",
      "105/105 [==============================] - 0s 592us/step - loss: 0.3765 - acc: 0.8667 - val_loss: 0.9538 - val_acc: 0.7037\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.90987\n",
      "Epoch 23/40\n",
      "105/105 [==============================] - 0s 569us/step - loss: 0.3505 - acc: 0.8857 - val_loss: 1.0382 - val_acc: 0.7407\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.90987\n",
      "Epoch 24/40\n",
      "105/105 [==============================] - 0s 587us/step - loss: 0.3477 - acc: 0.8857 - val_loss: 0.8481 - val_acc: 0.7407\n",
      "\n",
      "Epoch 00024: val_loss improved from 0.90987 to 0.84813, saving model to best_model.h5\n",
      "Epoch 25/40\n",
      "105/105 [==============================] - 0s 513us/step - loss: 0.2815 - acc: 0.9048 - val_loss: 0.7870 - val_acc: 0.7037\n",
      "\n",
      "Epoch 00025: val_loss improved from 0.84813 to 0.78700, saving model to best_model.h5\n",
      "Epoch 26/40\n",
      "105/105 [==============================] - 0s 615us/step - loss: 0.2437 - acc: 0.9333 - val_loss: 0.9357 - val_acc: 0.7407\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.78700\n",
      "Epoch 27/40\n",
      "105/105 [==============================] - 0s 663us/step - loss: 0.2478 - acc: 0.9333 - val_loss: 0.7671 - val_acc: 0.7407\n",
      "\n",
      "Epoch 00027: val_loss improved from 0.78700 to 0.76707, saving model to best_model.h5\n",
      "Epoch 28/40\n",
      "105/105 [==============================] - 0s 576us/step - loss: 0.2630 - acc: 0.8952 - val_loss: 1.1014 - val_acc: 0.6296\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.76707\n",
      "Epoch 29/40\n",
      "105/105 [==============================] - 0s 582us/step - loss: 0.2787 - acc: 0.8762 - val_loss: 0.9654 - val_acc: 0.7407\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 0.76707\n",
      "Epoch 30/40\n",
      "105/105 [==============================] - 0s 566us/step - loss: 0.2782 - acc: 0.8857 - val_loss: 0.9896 - val_acc: 0.7407\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.76707\n",
      "Epoch 31/40\n",
      "105/105 [==============================] - 0s 700us/step - loss: 0.1701 - acc: 0.9429 - val_loss: 0.8382 - val_acc: 0.6667\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 0.76707\n",
      "Epoch 32/40\n",
      "105/105 [==============================] - 0s 592us/step - loss: 0.2028 - acc: 0.9238 - val_loss: 0.7693 - val_acc: 0.6667\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 0.76707\n",
      "Epoch 33/40\n",
      "105/105 [==============================] - 0s 505us/step - loss: 0.1363 - acc: 0.9810 - val_loss: 0.7719 - val_acc: 0.7778\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 0.76707\n",
      "Epoch 34/40\n",
      "105/105 [==============================] - 0s 655us/step - loss: 0.0919 - acc: 0.9905 - val_loss: 0.8498 - val_acc: 0.7778\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 0.76707\n",
      "Epoch 35/40\n",
      "105/105 [==============================] - 0s 598us/step - loss: 0.1035 - acc: 0.9905 - val_loss: 0.8121 - val_acc: 0.7778\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 0.76707\n",
      "Epoch 36/40\n",
      "105/105 [==============================] - 0s 605us/step - loss: 0.0924 - acc: 0.9905 - val_loss: 0.7924 - val_acc: 0.7407\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 0.76707\n",
      "Epoch 37/40\n",
      "105/105 [==============================] - 0s 611us/step - loss: 0.0946 - acc: 0.9619 - val_loss: 0.8902 - val_acc: 0.7407\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 0.76707\n",
      "Epoch 38/40\n",
      "105/105 [==============================] - 0s 601us/step - loss: 0.1196 - acc: 0.9619 - val_loss: 1.0257 - val_acc: 0.7037\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 0.76707\n",
      "Epoch 39/40\n",
      "105/105 [==============================] - 0s 602us/step - loss: 0.2212 - acc: 0.9333 - val_loss: 1.1499 - val_acc: 0.7037\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 0.76707\n",
      "Epoch 40/40\n",
      "105/105 [==============================] - 0s 626us/step - loss: 0.1558 - acc: 0.9429 - val_loss: 1.2853 - val_acc: 0.5926\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 0.76707\n"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import EarlyStopping\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "checkpt = ModelCheckpoint(\"best_model.h5\", monitor='val_loss', verbose=True, save_best_only=True)\n",
    "earlystop = EarlyStopping(monitor='val_acc', patience=10)\n",
    "\n",
    "hist = model.fit(embed_matrix_train,YT,batch_size=32,epochs=40,shuffle=True,validation_split=0.2, callbacks=[checkpt, earlystop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4 3 2 2 2 2 1 2 4 2 1 2 0 2 1 3 2 2 3 2 0 0 4 2 3 1 2 0 1 2 0 1 0 2 0 1 2\n",
      " 3 4 2 1 0 0 1 2 2 2 2 0 1 1 0 3 2 3 0]\n"
     ]
    }
   ],
   "source": [
    "pred = model.predict_classes(embed_matrix_test)\n",
    "\n",
    "print(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "56/56 [==============================] - 0s 387us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.7502823386873518, 0.5714285969734192]"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(embed_matrix_test,Yt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I want to eat\n",
      "üç¥\n",
      "üç¥\n",
      "he did not answer\n",
      "üòì\n",
      "üòì\n",
      "he got a raise\n",
      "üòÅ\n",
      "üòÅ\n",
      "she got me a present\n",
      "‚ù§Ô∏è\n",
      "üòÅ\n",
      "ha ha ha it was so funny\n",
      "üòÅ\n",
      "üòÅ\n",
      "he is a good friend\n",
      "‚ù§Ô∏è\n",
      "üòÅ\n",
      "I am upset\n",
      "‚ù§Ô∏è\n",
      "‚öæ\n",
      "We had such a lovely dinner tonight\n",
      "‚ù§Ô∏è\n",
      "üòÅ\n",
      "where is the food\n",
      "üç¥\n",
      "üç¥\n",
      "Stop making this joke ha ha ha\n",
      "üòÅ\n",
      "üòÅ\n",
      "where is the ball\n",
      "‚öæ\n",
      "‚öæ\n",
      "work is hard\n",
      "üòì\n",
      "üòÅ\n",
      "This girl is messing with me\n",
      "üòì\n",
      "‚ù§Ô∏è\n",
      "are you serious ha ha\n",
      "üòÅ\n",
      "üòÅ\n",
      "Let us go play baseball\n",
      "‚öæ\n",
      "‚öæ\n",
      "This stupid grader is not working\n",
      "üòì\n",
      "üòì\n",
      "work is horrible\n",
      "üòì\n",
      "üòÅ\n",
      "Congratulation for having a baby\n",
      "üòÅ\n",
      "üòÅ\n",
      "stop messing around\n",
      "üòì\n",
      "üòì\n",
      "any suggestions for dinner\n",
      "üç¥\n",
      "üòÅ\n",
      "I love taking breaks\n",
      "‚ù§Ô∏è\n",
      "‚ù§Ô∏è\n",
      "you brighten my day\n",
      "üòÅ\n",
      "‚ù§Ô∏è\n",
      "I boiled rice\n",
      "üç¥\n",
      "üç¥\n",
      "she is a bully\n",
      "üòì\n",
      "üòÅ\n",
      "Why are you feeling bad\n",
      "üòì\n",
      "üòì\n",
      "I am upset\n",
      "üòì\n",
      "‚öæ\n",
      "I worked during my birthday\n",
      "üòì\n",
      "üòÅ\n",
      "My grandmother is the love of my life\n",
      "‚ù§Ô∏è\n",
      "‚ù§Ô∏è\n",
      "enjoy your break\n",
      "üòÅ\n",
      "‚öæ\n",
      "valentine day is near\n",
      "‚ù§Ô∏è\n",
      "üòÅ\n"
     ]
    }
   ],
   "source": [
    "for i in range(30):\n",
    "    print(' '.join(Xt[i]))\n",
    "    print(emoji.emojize(emoji_dictionary[str(np.argmax(Yt[i]))]))\n",
    "    print(emoji.emojize(emoji_dictionary[str(pred[i])]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Django application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"model.json\", \"w\") as file:\n",
    "    file.write(model.to_json())\n",
    "model.save_weights(\"model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import model_from_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"model.json\", \"r\") as file:\n",
    "    model=model_from_json(file.read())\n",
    "model.load_weights(\"model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.series.Series'>\n"
     ]
    }
   ],
   "source": [
    "test_str=\"hello how are you\"\n",
    "X = pd.Series(test_str)\n",
    "print(type(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getOutputEmbeddings(X):\n",
    "    \n",
    "#     embedding_matrix_output = np.zeros((X.shape[0],10,50))\n",
    "    \n",
    "#     for ix in range(X.shape[0]):\n",
    "#         X[ix] = X[ix].split()\n",
    "#         for jx in range(len(X[ix])):\n",
    "#             #go to every word in current(ix) sentence\n",
    "#             embedding_matrix_output[ix][jx] = embeddings[X[ix][jx].lower()]\n",
    "            \n",
    "    \n",
    "    X = X.split()\n",
    "    embedding_matrix_output = np.zeros((1,10,50))\n",
    "    for jx in range(len(X)):\n",
    "        #go to every word in current(ix) sentence\n",
    "        embedding_matrix_output[0][jx] = embeddings[X[jx].lower()]\n",
    "            \n",
    "    return embedding_matrix_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_X = getOutputEmbeddings(test_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = model.predict_classes(emb_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
